{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhbishtabhishek/Papers-Implemented/blob/main/ResNet_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quq4sjg25PeL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuB-dWrp5YN2"
      },
      "outputs": [],
      "source": [
        "class block(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, identity_downsamples=None, stride=1):\n",
        "    super(block, self).__init__()\n",
        "    self.expansion=4\n",
        "    self.conv1=nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "    self.bn1=nn.BatchNorm2d(out_channels)\n",
        "    self.conv2=nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "    self.bn2=nn.BatchNorm2d(out_channels)\n",
        "    self.conv3=nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "    self.bn3=nn.BatchNorm2d(out_channels*self.expansion)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.identity_downsamples=identity_downsamples\n",
        "\n",
        "  def forward(self, x):\n",
        "    identity=x\n",
        "\n",
        "    x=self.conv1(x)\n",
        "    x=self.bn1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.conv2(x)\n",
        "    x=self.bn2(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.conv3(x)\n",
        "    x=self.bn3(x)\n",
        "\n",
        "    if self.identity_downsamples is not None:\n",
        "      identity=self.identity_downsamples(identity)\n",
        "\n",
        "    x+=identity\n",
        "    x=self.relu(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo_0kJP3895b"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "  def __init__(self, block, layers, image_channels, num_classes):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels=64\n",
        "    self.conv1=nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "    self.bn1=nn.BatchNorm2d(64)\n",
        "    self.relu=nn.ReLU()\n",
        "    self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    #ResNet layers starting from here\n",
        "    self.layer1=self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
        "    self.layer2=self._make_layer(block, layers[1], out_channels=128, stride=2)\n",
        "    self.layer3=self._make_layer(block, layers[2], out_channels=256, stride=2)\n",
        "    self.layer4=self._make_layer(block, layers[3], out_channels=512, stride=2)\n",
        "\n",
        "    self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc=nn.Linear(512*4, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.conv1(x)\n",
        "    x=self.bn1(x)\n",
        "    x=self.relu(x)\n",
        "    x=self.maxpool(x)\n",
        "\n",
        "    x=self.layer1(x)\n",
        "    x=self.layer2(x)\n",
        "    x=self.layer3(x)\n",
        "    x=self.layer4(x)\n",
        "\n",
        "    x=self.avgpool(x)\n",
        "    x=x.reshape(x.shape[0], -1)\n",
        "    x=self.fc(x)\n",
        "    return x\n",
        "\n",
        "  def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n",
        "    identity_downsample=None\n",
        "    layers=[]\n",
        "\n",
        "    if stride!=1 or self.in_channels!=out_channels*4:\n",
        "      identity_downsample=nn.Sequential(nn.Conv2d(self.in_channels, out_channels*4, kernel_size=1, stride=stride), nn.BatchNorm2d(out_channels*4))\n",
        "\n",
        "      layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n",
        "      self.in_channels=out_channels*4\n",
        "\n",
        "      for i in range(num_residual_blocks-1):\n",
        "        layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvjPZ06sEB6J"
      },
      "outputs": [],
      "source": [
        "def ResNet50(img_channels=3, num_classes=1000):\n",
        "  return ResNet(block, [3,4,6,3], img_channels, num_classes)\n",
        "\n",
        "def ResNet101(img_channels=3, num_classes=1000):\n",
        "  return ResNet(block, [3,8,36,3], img_channels, num_classes)\n",
        "\n",
        "def ResNet152(img_channels=3, num_classes=1000):\n",
        "  return ResNet(block, [3,4,23,3], img_channels, num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auTkGaFtElpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d636e600-d5dd-42f6-a903-32739e18bd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1000])\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "  net=ResNet50()\n",
        "  x=torch.randn(2,3,224,224)\n",
        "  y=net(x).to('cuda')\n",
        "  print(y.shape)\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGvgzSTtImBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770cc2e6-52c5-4d68-b201-8f078e264233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1000])\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "  net=ResNet101()\n",
        "  x=torch.randn(2,3,224,224)\n",
        "  y=net(x).to('cuda')\n",
        "  print(y.shape)\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gm43AXHzImGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ee0de4-3f94-40bc-fac5-64b01ca4c838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1000])\n"
          ]
        }
      ],
      "source": [
        "def test():\n",
        "  net=ResNet152()\n",
        "  x=torch.randn(2,3,224,224)\n",
        "  y=net(x).to('cuda')\n",
        "  print(y.shape)\n",
        "\n",
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMhou0zkCNxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a9737c-4d52-42c4-cf4c-9c30552f87fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet-50 params: 25583592\n",
            "ResNet-101 params: 60268520\n",
            "ResNet-152 params: 44601832\n"
          ]
        }
      ],
      "source": [
        "net = ResNet50()\n",
        "print(\"ResNet-50 params:\", sum(p.numel() for p in net.parameters()))\n",
        "\n",
        "net = ResNet101()\n",
        "print(\"ResNet-101 params:\", sum(p.numel() for p in net.parameters()))\n",
        "\n",
        "net = ResNet152()\n",
        "print(\"ResNet-152 params:\", sum(p.numel() for p in net.parameters()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N3asFvrC-FT"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkjo1aDZC5w6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d55b322-0840-4c09-cd42-5aea1469cf75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XgNhw8fC_dW"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = ResNet50(img_channels=3, num_classes=10).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pESAf7BDBtU"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wQI-QCyDDaC"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(net, trainloader, optimizer, criterion, device, epoch):\n",
        "    net.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    loop = tqdm(trainloader, leave=True)\n",
        "    for inputs, targets in loop:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        loop.set_description(f\"Epoch [{epoch}]\")\n",
        "        loop.set_postfix(loss=running_loss/total, acc=100.*correct/total)\n",
        "\n",
        "    return running_loss/len(trainloader), 100.*correct/total\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhQTSWGA5fM8"
      },
      "outputs": [],
      "source": [
        "def test(net, testloader, criterion, device):\n",
        "    net.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(testloader)\n",
        "    accuracy = 100. * correct / total\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZnEV7WR1dR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef5fba9-d35c-4c9b-904a-65860b0976d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQy8olURDFP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29663ceb-4249-482f-a057-ee920128ab42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [96]: 100%|██████████| 391/391 [10:22<00:00,  1.59s/it, acc=100, loss=2.09e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 96/100 Train Loss: 0.0027 | Train Acc: 99.95% Test Loss: 0.5251 | Test Acc: 89.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [97]: 100%|██████████| 391/391 [10:25<00:00,  1.60s/it, acc=100, loss=1.93e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 97/100 Train Loss: 0.0025 | Train Acc: 99.97% Test Loss: 0.5081 | Test Acc: 89.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [98]: 100%|██████████| 391/391 [10:24<00:00,  1.60s/it, acc=100, loss=1.98e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 98/100 Train Loss: 0.0025 | Train Acc: 99.98% Test Loss: 0.5137 | Test Acc: 90.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [99]: 100%|██████████| 391/391 [10:23<00:00,  1.59s/it, acc=100, loss=2.01e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 99/100 Train Loss: 0.0026 | Train Acc: 99.97% Test Loss: 0.4816 | Test Acc: 90.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [100]: 100%|██████████| 391/391 [10:24<00:00,  1.60s/it, acc=100, loss=2.18e-5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/100 Train Loss: 0.0028 | Train Acc: 99.96% Test Loss: 0.4909 | Test Acc: 90.16%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/checkpoints/resnet_checkpoint.pth\"\n",
        "\n",
        "start_epoch = 0\n",
        "if os.path.isfile(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    net.load_state_dict(checkpoint['model_state'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(start_epoch + 1, num_epochs + 1):\n",
        "    train_loss, train_acc = train(net, trainloader, optimizer, criterion, device, epoch)\n",
        "    test_loss, test_acc = test(net, testloader, criterion, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}/{num_epochs} \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% \"\n",
        "          f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'model_state': net.state_dict(),\n",
        "        'optimizer_state': optimizer.state_dict(),\n",
        "        'scheduler_state': scheduler.state_dict()\n",
        "    }\n",
        "    torch.save(state, checkpoint_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNCkGP0obsqInTjdGqvhKpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
